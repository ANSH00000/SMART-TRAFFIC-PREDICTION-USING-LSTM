"""
data_processing.py

Robust loader + pipeline that uses the explicit CSV path:
    /content/TrafficTwoMonth.csv

It prints a clear preview and either processes the file or shows
helpful diagnostics if the file contents are unexpected.
"""

from typing import List, Optional, Tuple
import os
import json
import sys
import csv
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import joblib


def try_read_csv_with_sniff(path: str) -> pd.DataFrame:
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        sample = f.read(8192)
        f.seek(0)
        try:
            dialect = csv.Sniffer().sniff(sample)
            sep = dialect.delimiter
        except Exception:
            sep = None
    if sep:
        try:
            return pd.read_csv(path, sep=sep)
        except Exception:
            pass
    return pd.read_csv(path)


def try_read_json(path: str) -> Optional[pd.DataFrame]:
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            first = f.readline().strip()
            f.seek(0)
            # json-lines heuristic
            if first.startswith("{") and "\n" in open(path, "r", encoding="utf-8", errors="replace").read():
                records = []
                for line in open(path, "r", encoding="utf-8", errors="replace"):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        records.append(json.loads(line))
                    except Exception:
                        records = None
                        break
                if records:
                    return pd.DataFrame.from_records(records)
            f.seek(0)
            data = json.load(f)
            if isinstance(data, list):
                return pd.DataFrame.from_records(data)
            if isinstance(data, dict):
                return pd.json_normalize(data)
    except Exception:
        return None
    return None


def load_csv(path: str) -> pd.DataFrame:
    """Robust loader: tries CSV, JSON-lines, sniffed CSV, headerless CSV."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"File not found: {path}")

    # 1) Try pandas default
    try:
        df = pd.read_csv(path)
    except Exception:
        df = None

    # 2) If single column that looks JSON-like, try JSON parsing
    if df is not None and df.shape[1] == 1:
        only_col = df.columns[0]
        sample_vals = df[only_col].astype(str).head(5).str.strip().tolist()
        looks_json = any(s.startswith("{") or s.startswith("[") for s in sample_vals)
        if looks_json:
            df_json = try_read_json(path)
            if df_json is not None and df_json.shape[1] > 1:
                df = df_json

    # 3) Try sniffed CSV if still single-column or read failed
    if df is None or df.shape[1] == 1:
        try:
            df2 = try_read_csv_with_sniff(path)
            if df2 is not None:
                if df is None or df2.shape[1] >= df.shape[1]:
                    df = df2
        except Exception:
            pass

    # 4) Try header=None fallback
    if df is not None and df.shape[1] == 1:
        try:
            df3 = pd.read_csv(path, header=None)
            if df3.shape[1] > 1:
                df3.columns = [f"col_{i}" for i in range(df3.shape[1])]
                df = df3
        except Exception:
            pass

    if df is None:
        raise ValueError("Unable to read the file into a DataFrame.")
    return df


def basic_cleaning(df: pd.DataFrame, fill_method: str = "ffill") -> pd.DataFrame:
    df = df.copy()
    df = df.drop_duplicates().reset_index(drop=True)
    if fill_method == "ffill":
        df = df.ffill().bfill()
    else:
        df = df.fillna(0)
    return df


def add_time_features(df: pd.DataFrame, time_col: Optional[str] = None) -> pd.DataFrame:
    df = df.copy()
    if time_col and time_col in df.columns:
        ts = pd.to_datetime(df[time_col], errors="coerce")
        if ts.notna().any():
            df["hour"] = ts.dt.hour
            df["minute"] = ts.dt.minute
            df["dayofweek"] = ts.dt.dayofweek
            df["day"] = ts.dt.day
            df["month"] = ts.dt.month
            df["is_weekend"] = ts.dt.dayofweek.isin([5, 6]).astype(int)
            return df
    # fallback placeholders
    df["hour"] = 0
    df["minute"] = 0
    df["dayofweek"] = 0
    df["day"] = 0
    df["month"] = 0
    df["is_weekend"] = 0
    return df


def scale_features(df: pd.DataFrame, feature_cols: List[str], scaler_path: Optional[str] = None, scaler: Optional[MinMaxScaler] = None) -> Tuple[pd.DataFrame, MinMaxScaler]:
    df_scaled = df.copy()
    if len(feature_cols) == 0:
        raise ValueError("feature_cols must contain at least one column name")
    if scaler is None:
        scaler = MinMaxScaler()
        scaled = scaler.fit_transform(df_scaled[feature_cols].values)
    else:
        scaled = scaler.transform(df_scaled[feature_cols].values)
    df_scaled[feature_cols] = scaled
    if scaler_path:
        joblib.dump(scaler, scaler_path)
    return df_scaled, scaler


def create_sequences(df: pd.DataFrame, feature_cols: List[str], target_col: str, seq_len: int = 12, horizon: int = 1) -> Tuple[np.ndarray, np.ndarray]:
    if seq_len < 1:
        raise ValueError("seq_len must be >= 1")
    if horizon < 1:
        raise ValueError("horizon must be >= 1")
    data = df[feature_cols].values
    targets = df[target_col].values
    n_total = len(data)
    X, y = [], []
    for i in range(n_total - seq_len - horizon + 1):
        X.append(data[i : i + seq_len])
        y.append(targets[i + seq_len + horizon - 1])
    return np.asarray(X), np.asarray(y)


def train_val_test_split(X: np.ndarray, y: np.ndarray, val_size: float = 0.2, test_size: float = 0.1, random_state: int = 42, shuffle: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    if not (0 <= val_size < 1) or not (0 <= test_size < 1):
        raise ValueError("val_size and test_size must be between 0 and 1")
    if val_size + test_size >= 1.0:
        raise ValueError("val_size + test_size must be less than 1.0")
    n = len(X)
    if n == 0:
        return (np.empty((0,) + X.shape[1:]), np.empty((0,) + X.shape[1:]), np.empty((0,) + X.shape[1:]), np.empty((0,)), np.empty((0,)), np.empty((0,)))
    train_frac = 1.0 - (val_size + test_size)
    if shuffle:
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_frac, random_state=random_state, shuffle=True)
    else:
        n_train = int(np.floor(train_frac * n))
        X_train, X_temp = X[:n_train], X[n_train:]
        y_train, y_temp = y[:n_train], y[n_train:]
    if len(X_temp) == 0:
        X_val = np.empty((0,) + X.shape[1:])
        X_test = np.empty((0,) + X.shape[1:])
        y_val = np.empty((0,))
        y_test = np.empty((0,))
        return X_train, X_val, X_test, y_train, y_val, y_test
    val_frac_of_temp = val_size / (val_size + test_size)
    if shuffle:
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=val_frac_of_temp, random_state=random_state, shuffle=True)
    else:
        n_temp = len(X_temp)
        n_val = int(np.floor(val_frac_of_temp * n_temp))
        X_val, X_test = X_temp[:n_val], X_temp[n_val:]
        y_val, y_test = y_temp[:n_val], y_temp[n_val:]
    return X_train, X_val, X_test, y_train, y_val, y_test


# ------------------ MAIN ------------------

if __name__ == "__main__":
    # <- You gave this exact path. We will use it and only it.
    CSV_PATH = "/content/TrafficTwoMonth.csv"

    print("Attempting to load:", CSV_PATH)
    if not os.path.exists(CSV_PATH):
        print(f"File not found at {CSV_PATH}. Please upload the CSV to that path (e.g., in Colab use files.upload or mount Drive).")
        raise FileNotFoundError(CSV_PATH)

    df_raw = load_csv(CSV_PATH)
    print("Raw shape:", df_raw.shape)
    print("Raw columns:", list(df_raw.columns))
    print("\n--- File preview (first 10 rows) ---")
    print(df_raw.head(10).to_string(index=False))

    # Detect time-like column (require >= half non-null parsed values)
    time_col = None
    for c in df_raw.columns:
        try:
            converted = pd.to_datetime(df_raw[c], errors="coerce")
            if converted.notna().sum() >= max(1, len(df_raw) // 2):
                time_col = c
                df_raw[c] = converted
                break
        except Exception:
            continue

    if time_col:
        print("\nDetected time column:", time_col)
    else:
        print("\nNo time column detected automatically (time features will be placeholders).")

    # Clean, add time features
    df = basic_cleaning(df_raw)
    df = add_time_features(df, time_col=time_col)

    # Choose a numeric target
    target_col = None
    for candidate in ["traffic", "flow", "vehicle_count", "count", "vehicles", "value"]:
        if candidate in df.columns:
            target_col = candidate
            break

    if target_col is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        # ignore engineered time cols
        for t in ["hour", "minute", "dayofweek", "day", "month", "is_weekend"]:
            if t in numeric_cols:
                numeric_cols.remove(t)
        if len(numeric_cols) > 0:
            target_col = numeric_cols[0]

    if target_col is None:
        print("\nERROR: No numeric column found to act as target. Available columns and dtypes:")
        print(df.dtypes)
        print("\nPlease inspect the file preview above. If your data is JSON inside a single column, paste 3â€“5 rows here and I will parse it for you.")
        raise SystemExit(1)

    print("\nSelected target column:", target_col)

    # Build feature columns (time features + numeric cols)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    base_time_feats = [c for c in ["hour", "is_weekend"] if c in df.columns]
    feature_cols = base_time_feats + [c for c in numeric_cols if c not in base_time_feats]
    seen = set()
    feature_cols = [x for x in feature_cols if not (x in seen or seen.add(x))]
    if target_col not in feature_cols:
        feature_cols.append(target_col)

    print("Feature columns chosen:", feature_cols)

    # Scale
    try:
        df_scaled, scaler = scale_features(df, feature_cols=feature_cols)
    except Exception as e:
        print("Scaling failed:", e)
        print("Data types and preview:")
        print(df.dtypes)
        print(df.head(10).to_string(index=False))
        raise SystemExit(1)

    # Adaptive seq_len for short datasets
    min_required = 5
    max_seq = max(1, len(df_scaled) // 4)
    seq_len = min(12, max_seq) if len(df_scaled) >= min_required else 1
    seq_len = max(1, seq_len)
    horizon = 1

    print(f"\nUsing seq_len={seq_len}, horizon={horizon}, dataset length={len(df_scaled)}")

    X, y = create_sequences(df_scaled, feature_cols=feature_cols, target_col=target_col, seq_len=seq_len, horizon=horizon)
    print(f"Created sequences: X.shape={X.shape}, y.shape={y.shape}")
    if X.size == 0:
        print("No sequences created. Try increasing dataset length or verify the CSV contents.")
        raise SystemExit(0)

    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, val_size=0.2, test_size=0.1, shuffle=False)
    print("Train/Val/Test shapes (X):", X_train.shape, X_val.shape, X_test.shape)
    print("Train/Val/Test shapes (y):", y_train.shape, y_val.shape, y_test.shape)

    print("\nSample X[0]:")
    print(X[0])
    print("y[0]:", y[0])

    # Save scaler
    try:
        scaler_save_path = os.path.join(os.path.dirname(CSV_PATH) or ".", "minmax_scaler.joblib")
        joblib.dump(scaler, scaler_save_path)
        print("\nSaved scaler to:", scaler_save_path)
    except Exception as e:
        print("Couldn't save scaler:", e)
